---
layout: "../../layouts/BlogPost.astro"
title: "chatbot using TensorFlow and libdevice fix"
description: "In this article, we will be creating a chatbot using TensorFlow and libdevice fix with a dataset of pirate-related phrases."
updatedDate: "Jan 19 2023"
pubDate: "Jan 19 2023"
author: "Miguel Gargallo"
tags: ["chatbot", "tensorflow", "libdevice"]
heroImage: "https://raw.githubusercontent.com/superdatas/blog-images/main/ai-advancement-science.png"
language: "en"
---

In this article, we will be creating a chatbot using TensorFlow and a libdevice fix with a dataset of pirate-related phrases. The chatbot will be able to respond to various inputs related to pirates, such as questions about pirate history or requests for pirate jokes.

To start, we will import the necessary libraries, including TensorFlow, NumPy, Pandas, and NLTK. We will also use the json library to load the dataset and the Tokenizer class from TensorFlow to tokenize the input data.

Next, we will load the dataset and extract the input phrases and corresponding tags (or categories) into separate lists. We will then convert these lists into a Pandas dataframe for easier manipulation.

After cleaning up the data by removing punctuation and converting the input phrases to lowercase, we will tokenize the input phrases using the Tokenizer class. We will also apply padding to the tokenized input phrases to ensure that all input sequences have the same length.

Next, we will encode the output tags using the LabelEncoder class from scikit-learn. We will then define the vocabulary size and output length based on the tokenized input phrases and encoded output tags.

With the data preprocessed, we can then create the chatbot model using TensorFlow. The model will consist of an input layer, an embedding layer, an LSTM layer, a flattening layer, and a dense layer with a softmax activation function. We will then compile the model and train it on the preprocessed data.

Finally, we will plot the model's accuracy and loss over the course of the training and use the trained model to chat with the user. The user can input a phrase related to pirates and the chatbot will respond with an appropriate response.

Here you will be able to see the error code: "libdevice not found" error that was encountered while training the model.

<pre>
```python
#importing the libraries
import tensorflow as tf
import numpy as np
import pandas as pd
import json
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, GlobalMaxPooling1D, Flatten
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt

#importing the dataset
with open('content.json') as content:
  data1 = json.load(content)

#getting all the data to lists
tags = []
inputs = []
responses={}
for intent in data1['intents']:
  responses[intent['tag']]=intent['responses']
  for lines in intent['input']:
    inputs.append(lines)
    tags.append(intent['tag'])

#converting to dataframe
data = pd.DataFrame({"inputs":inputs,
                     "tags":tags})

#printing the data
data

data = data.sample(frac=1)

#removing punctuations
import string
data['inputs'] = data['inputs'].apply(lambda wrd:[ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])
data['inputs'] = data['inputs'].apply(lambda wrd: ''.join(wrd))
data

#tokenize the data
from tensorflow.keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer(num_words=2000)
tokenizer.fit_on_texts(data['inputs'])
train = tokenizer.texts_to_sequences(data['inputs'])
#apply padding
from tensorflow.keras.preprocessing.sequence import pad_sequences
x_train = pad_sequences(train)

#encoding the outputs
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(data['tags'])

input_shape = x_train.shape[1]
print(input_shape)

#define vocabulary
vocabulary = len(tokenizer.word_index)
print("number of unique words : ",vocabulary)
output_length = le.classes_.shape[0]
print("output length: ",output_length)

#creating the model

i = Input(shape=(input_shape,))
x = Embedding(vocabulary+1,10)(i)
x = LSTM(10,return_sequences=True)(x)
x = Flatten()(x)
x = Dense(output_length,activatiimport pandas as pd
import json
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, GlobalMaxPooling1D, Flatten
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt

model.compile(loss="sparse_categorical_crossentropy",optimizer='adam',metrics=['accuracy'])

#training the model
train = model.fit(x_train,y_train,epochs=200)

#plotting model accuracy
plt.plot(train.history['accuracy'],label='training set accuracy')
plt.plot(train.history['loss'],label='training set loss')
plt.legend()

#chatting
import random


while True:
  texts_p = []
  prediction_input = input('You : ')

  #removing punctuation and converting to lowercase
  prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]
  prediction_input = ''.join(prediction_input)
  texts_p.append(prediction_input)

  #tokenizing and padding
  prediction_input = tokenizer.texts_to_sequences(texts_p)
  prediction_input = np.array(prediction_input).reshape(-1)
  prediction_input = pad_sequences([prediction_input],input_shape)

  #getting output from model
  output = model.predict(prediction_input)
  output = output.argmax()

  #finding the right tag and predicting
  response_tag = le.inverse_transform([output])[0]
  print("Going Merry : ",random.choice(responses[response_tag]))
  if response_tag == "goodbye":
    break
```

Also I got an error, when I tried to run the code. I got the following error:

```
#training the model
train = model.fit(x_train,y_train,epochs=200)

Epoch 1/200

2023-01-19 14:07:32.719434: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-01-19 14:07:32.719780: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc
2023-01-19 14:07:32.734790: W tensorflow/compiler/xla/service/gpu/llvm_gpu_backend/gpu_backend_lib.cc:326] libdevice is required by this HLO module but was not found at ./libdevice.10.bc
2023-01-19 14:07:32.735059: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at xla_ops.cc:446 : INTERNAL: libdevice not found at ./libdevice.10.bc

---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
Cell In[131], line 2
      1 #training the model
----> 2 train = model.fit(x_train,y_train,epochs=200)

File ~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70, in filter_traceback.<locals>.error_handler(*args, **kwargs)
     67     filtered_tb = _process_traceback_frames(e.__traceback__)
     68     # To get the full stack trace, call:
     69     # `tf.debugging.disable_traceback_filtering()`
---> 70     raise e.with_traceback(filtered_tb) from None
     71 finally:
     72     del filtered_tb

File ~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     50 try:
     51   ctx.ensure_initialized()
---> 52   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     53                                       inputs, attrs, num_outputs)
     54 except core._NotOkStatusException as e:
     55   if name is not None:

InternalError: Graph execution error:

Detected at node 'StatefulPartitionedCall_5' defined at (most recent call last):
    File "/usr/lib/python3.10/runpy.py", line 196, in _run_module_as_main
      return _run_code(code, main_globals, None,
    File "/usr/lib/python3.10/runpy.py", line 86, in _run_code
      exec(code, run_globals)
    File "/usr/lib/python3.10/site-packages/ipykernel_launcher.py", line 17, in <module>
      app.launch_new_instance()
    File "/usr/lib/python3.10/site-packages/traitlets/config/application.py", line 1041, in launch_instance
      app.start()
    File "/usr/lib/python3.10/site-packages/ipykernel/kernelapp.py", line 724, in start
      self.io_loop.start()
    File "/usr/lib/python3.10/site-packages/tornado/platform/asyncio.py", line 215, in start
      self.asyncio_loop.run_forever()
    File "/usr/lib/python3.10/asyncio/base_events.py", line 603, in run_forever
      self._run_once()
    File "/usr/lib/python3.10/asyncio/base_events.py", line 1906, in _run_once
      handle._run()
    File "/usr/lib/python3.10/asyncio/events.py", line 80, in _run
      self._context.run(self._callback, *self._args)
    File "/usr/lib/python3.10/site-packages/ipykernel/kernelbase.py", line 512, in dispatch_queue
      await self.process_one()
    File "/usr/lib/python3.10/site-packages/ipykernel/kernelbase.py", line 501, in process_one
      await dispatch(*args)
    File "/usr/lib/python3.10/site-packages/ipykernel/kernelbase.py", line 408, in dispatch_shell
      await result
    File "/usr/lib/python3.10/site-packages/ipykernel/kernelbase.py", line 731, in execute_request
      reply_content = await reply_content
    File "/usr/lib/python3.10/site-packages/ipykernel/ipkernel.py", line 417, in do_execute
      res = shell.run_cell(
    File "/usr/lib/python3.10/site-packages/ipykernel/zmqshell.py", line 540, in run_cell
      return super().run_cell(*args, **kwargs)
    File "/usr/lib/python3.10/site-packages/IPython/core/interactiveshell.py", line 2945, in run_cell
      result = self._run_cell(
    File "/usr/lib/python3.10/site-packages/IPython/core/interactiveshell.py", line 3000, in _run_cell
      return runner(coro)
    File "/usr/lib/python3.10/site-packages/IPython/core/async_helpers.py", line 129, in _pseudo_sync_runner
      coro.send(None)
    File "/usr/lib/python3.10/site-packages/IPython/core/interactiveshell.py", line 3203, in run_cell_async
      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,
    File "/usr/lib/python3.10/site-packages/IPython/core/interactiveshell.py", line 3382, in run_ast_nodes
      if await self.run_code(code, result, async_=asy):
    File "/usr/lib/python3.10/site-packages/IPython/core/interactiveshell.py", line 3442, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File "/tmp/ipykernel_40257/48579934.py", line 2, in <module>
      train = modeimport pandas as pd
import json
import nltk
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, GlobalMaxPooling1D, Flatten
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt
.fit(x_train,y_train,epochs=200)
    File "/home/fengh/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py", line 65, in error_handler
      return fn(*args, **kwargs)
    File "/home/fengh/.local/lib/python3.10/site-packages/keras/engine/training.py", line 1685, in fit
      tmp_logs = self.train_function(iterator)
    File "/home/fengh/.local/lib/python3.10/site-packages/keras/engine/training.py", line 1284, in train_function
      return step_function(self, iterator)
    File "/home/fengh/.local/lib/python3.10/site-packages/keras/engine/training.py", line 1268, in step_function
      outputs = model.distribute_strategy.run(run_step, args=(data,))
    File "/home/fengh/.local/lib/python3.10/site-packages/keras/engine/training.py", line 1249, in run_step
      outputs = model.train_step(data)
    File "/home/fengh/.local/lib/python3.10/site-packages/keras/engine/training.py", line 1054, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File "/home/fengh/.local/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 532, in minimize
      self.apply_gradients(grads_and_vars)
    File "/home/fengh/.local/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1163, in apply_gradients
      return super().apply_gradients(grads_and_vars, name=name)
    File "/home/fengh/.local/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 639, in apply_gradients
      iteration = self._internal_apply_gradients(grads_and_vars)
    File "/home/fengh/.local/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1189, in _internal_apply_gradients
      return tf.__internal__.distribute.interim.maybe_merge_call(
    File "/home/fengh/.local/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1239, in _distributed_apply_gradients_fn
      distribution.extended.update(
    File "/home/fengh/.local/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 1234, in apply_grad_to_update_var
      return self._update_step_xla(grad, var, id(self._var_key(var)))
Node: 'StatefulPartitionedCall_5'
libdevice not found at ./libdevice.10.bc
	 [[{{node StatefulPartitionedCall_5}}]] [Op:__inference_train_function_30698]
```
</pre>


To fix this issue for the "libdevice not found" error is to locate the "libdevice" library file and place it in the correct directory. To do this, the user can run the command find / -type d -name nvvm 2>/dev/null on the console. This command will search for the directory named "nvvm" on the user's system.

Once the location of the "nvvm" directory is found, the user should check if it is located at /usr/lib/nvvm/. If it is not, the user should create a directory at /usr/lib/nvvm/ and place the file "libdevice.10.bc" there.

The user should also place the file "libdevice.10.bc" in the project directory where the error is occurring. This additional step ensures that the library file is accessible to TensorFlow when running the code.

By following these steps, the user can solve the "libdevice not found" error that was encountered while training the model. It is important to note that I did not a solution to fix the "libdevice not found" error on my system. I was able to fix the error by following the steps outlined in this article. If you need help, contact me on twitter, feel free to ask me for support!

Overall, this article demonstrates how TensorFlow can be used to create a chatbot using a dataset of pirate-related phrases. The chatbot is able to respond to various inputs in a conversational manner, making it a useful tool for pirate enthusiasts.

<div>
  <a href="https://colab.research.google.com/github/miguelgargallo/jupyter-notebook/blob/main/03-tensorflow-pirates-chatbot/03-tensorflow-pirates-chatbot.ipynb" target="_blank" rel="noopener noreferrer">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
  </a>
  <a> and </a>
  <a
    href="https://github.com/miguelgargallo/jupyter-notebook/tree/main/03-tensorflow-pirates-chatbot" target="_blank" rel="noopener noreferrer">
    <img src="https://github.githubassets.com/images/modules/logos_page/GitHub-Logo.png"
    width="100" height="20" alt="View on GitHub"/>
  </a>
</div>
